# 数值稳定性
# 梯度爆炸：1.5^100=非常大的数，可能超出float，100表示网络很深
## 梯度爆炸后，lr太小训练太慢，lr太大爆炸，需要在训练过程中不断调整学习率
## fp16训练快，但是很可能超出范围，梯度爆炸

# 梯度消失：0.8^100=非常小的数
## sigmoid，输入大点的时候，梯度很小，当层数深的时候，梯度消失
## 梯度消失后，训练没有进展，仅仅顶部训练效果好，底部尤为严重


# 训练稳定：让梯度值在合理范围内
# 1. 乘法变加法
# 2. 归一化、裁剪
# 3. 合理权重初始化和激活函数
# 3 展开
# 希望每层的输出和梯度都是稳定的，即均值方差一定
# 初始化：基于上述假设，可以求得初始化时的公式，即xavier方法
# 激活：基于上述假设，激活函数要近似于y=x，所以tanh和relu效果不错，sigmoid可以*4-1（根据泰勒展开）近似y=x
