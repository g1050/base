

def conv_layer():
    # 两个原则
    # 平移不变性质
    # 局部性

    # 卷积是一个特殊的全连接层
    # 对全连接层使用平移不变性和局部性得到卷积层

    # 交叉相关vs卷积

    # 卷积中：
    ## 超参数：核大小
    ## 参数：核矩阵weight和bias
    ## 卷积解决了全连接网络，参数量随着输入的增大而增大

    # 卷积运算的输出大小：
    ## h_out = h_src-h_kernel + 1
    ## w_out = w_src-w_kernel + 1

    # python中的[:] inplace原地操作

    # todo:如何通过人工构造数据集，学习如何学习卷积核的权重
    pass


def stride_padding():
    # 每次卷积，特征图都会变小，若干次之后便不能继续卷积，如何加深网络，实现深度学习
    # 1. 填充
    # h_out = h_src-h_kernel + 1 + ph(高度上填充的)
    # w_out = w_src-w_kernel + 1 + pw
    # 当ph = h_kernel - 1 时候不改变输出形状，所以当核大小为3的时候填充2，即上下各填充1，所以卷积核一般取奇数

    # 2. 步幅
    # 输入太大的时候，需要很多层卷积才能将特征图缩小到很小程度，所以添加步幅可以加快缩小特征，减少计算
    # h_out = [h_src-h_kernel + 1 + ph(高度上填充的) + s -1] /s 除以s表示向下取整

    # 超参数：核大小、填充（核-1），步幅（选1最好，但是计算量太大）
    pass