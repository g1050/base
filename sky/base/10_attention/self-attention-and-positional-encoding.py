# 注意力池化层也是一种用来抽取特征的方式
# t时刻的q，k，v是同一个东西即输入x，称为自注意力机制

# todo： CNN、RNN、self-attention对比
# 自注意力机制：擅长长文本处理，因为看到的视野比较广，带来的计算复杂度也比较高

# 自注意力机制没有记录位置信息，所以添加位置编码，直接把位置信息和输入相加，直接添加到输入里面
# 位置矩阵编码：用的正弦函数，编码编的是微信的相对信息