# todo: batch norm、layer norm
# BN：在feature维度对样本做Normlization
# LN: 
# XXnomlization

# 数据在底层，最上层是loss损失函数
# 由于backward是forward的反向，所以上层梯度大（乘的数少），下层梯度小
# 所以下层在一直变化，下层一变化上层就要重新训练

# 操作：对某个小批量 减均值，除以方差，其中均值方差是根据样本来的
# 公式 r(x-u)/sigma + b 其中r和b为可以学习的参数

# 对fc: 作用在特征维度，是一列一列求均值，比如[[1,2],[3,4]] => [[(1+3)/2,(2+4)/2]]
# 对conv：作用在通道维

# 后来的研究发现：BN实际上是在每个小批量里面加入噪音，控制模型复杂度
# 固定住小批量的均值和方差，然后学习r和b，加快了模型收敛，精度一般没有提升

# 归一化：推理使用全局的mean，推理使用小批量的归一化

# BN要加在激活函数之前

# BN：线性变化+归一化